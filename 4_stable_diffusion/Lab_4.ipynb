{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f671ae67-f9fe-4f36-a40c-4911a50504d8",
   "metadata": {},
   "source": [
    "# Stable Diffusion with Distributed Training and Hosting on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c5040-dfde-4a3f-ab62-91d107b28c5c",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how you can fine-tune a pretrained [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release) model on SageMaker and deploy it for inference.\n",
    "\n",
    "Produced by Stability.ai, Stable Diffusion is an open source model available for researchers and the broader ML community. We're pointing to the core content available on Hugging Face [here](https://huggingface.co/CompVis/stable-diffusion-v1-4) and provide private access in the limited context of hands-on workshops. If you'd like longer term access to Stable Diffusion, you'll need to sign up on the Hugging Face Hub, accept the terms, create a token, and download the model and dataset. \n",
    "\n",
    "In this lab, we've done that for you already. So let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75580a08-54ce-4f55-a7ea-b34eaf1d6fc9",
   "metadata": {},
   "source": [
    "This notebook is purely educational for showing how to fine-tune latent-stable-diffusion on Amazon SageMaker. Neither the images produced or code represent Amazon or its views in any way shape or form. To properly leverage this codebase, read the corresponding licenses from [CompVis](https://huggingface.co/spaces/CompVis/stable-diffusion-license) (the model) and [Conceptual Captions](https://huggingface.co/datasets/conceptual_captions) (from Google, but you will use HF)\n",
    "\n",
    "Model weights were provided by CompVis/stable-diffusion-v1-4. You can find the licensing, README and more [here](https://huggingface.co/CompVis/stable-diffusion-v1-4).  Please note that the finetune.py script has been slightly modified from a PR request [here](https://github.com/huggingface/diffusers/pull/356)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870350f-5d95-4dba-a008-6ed946b18ac1",
   "metadata": {},
   "source": [
    "### Step 1. Point to the model and data in S3\n",
    "\n",
    "First, ask the instructor for the name of the S3 bucket you need to point to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f446849-ea83-4d8a-822d-b75f54cc63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_bucket = \" \" #please ask your instructor to provide the name of this private bucket \n",
    "path = \"conceptual_captions\"\n",
    "lab_data = f\"s3://{lab_bucket}/{path}\"\n",
    "image_file = '0.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14188fb-2e5d-4ac1-aa13-d452a78e0370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp {lab_data}/{image_file} .\n",
    "!aws s3 cp {lab_data}/dataset.parquet ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc4db69-8b8c-4520-a623-02ef6287ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4862f17-4f0a-4b2d-af8e-1ab628eb9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64398d0a-d96c-49e3-aed0-222f3ad4bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = df['caption'][0]\n",
    "from PIL import Image\n",
    "\n",
    "print (caption)\n",
    "\n",
    "Image.open(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d18636-1fb9-4a64-81a3-a08a08a7a49d",
   "metadata": {},
   "source": [
    "### Step 1.5 Copy the data into your SageMaker Session Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8920b65f-e77c-4c81-b5c5-c2ba76cefa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f8b3b9-4bb2-4b25-bd41-12484c535929",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_channel = \"s3://{}/conceptual_captions\".format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc804a34-492e-4dc5-ba8e-0fa46fff9886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 sync {lab_data} {s3_train_channel}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962a94b-bf13-4154-8973-a9fe8f30e7c3",
   "metadata": {},
   "source": [
    "Additionally, the data you will be using comes from mscoco. However, you can also download from [here](https://huggingface.co/datasets/ChristophSchuhmann/MS_COCO_2017_URL_TEXT) which uses the dataset from [here](https://academictorrents.com/details/74dec1dd21ae4994dfd9069f9cb0443eb960c962). Then use this [link](https://github.com/rom1504/img2dataset) to quickly fill in the datasets files. For the purpose of this notebook you can download a few samples using the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6c915-3ef8-4433-8628-b85c799a9873",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2. Run distributed training on Amazon SageMaker\n",
    "Next, let's configure the scripts to run on SageMaker training jobs with high performance GPU's. First, you'll need to determine which instances to use. We'd suggest you start with the `ml.g5.12xlarge`, which has 4 GPUs and is known to work nicely with this training script and dataset.\n",
    "\n",
    "The training script we're working with today uses Hugging Face's [`accelerate`](https://huggingface.co/docs/accelerate/index) library to run data parallel on all available GPUs. While likely not as performant on AWS as [SageMaker Distributed Data Parallel](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html), it's still an easy and efficient way to run data parallel on SageMaker Training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f05777-e2a5-480d-a16f-56e95500320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processes_per_host(instance_type):\n",
    "    if instance_type in ['ml.g5.12xlarge', 'ml.g5.24xlarge', 'ml.p3.8xlarge']:\n",
    "        processes_per_host = 4\n",
    "        \n",
    "    elif instance_type in ['ml.g5.48xlarge', 'ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge']:\n",
    "        processes_per_host = 8\n",
    "        \n",
    "    elif instance_type == 'ml.g4dn.12xlarge':\n",
    "        print ('Stable diffusion is known to not run nicely on the g4dn.12xlarge, recommend a reset to g5.12xlarge')\n",
    "    \n",
    "    else:\n",
    "        print ('Please look up the number of GPUs per node from the EC2 page here: https://aws.amazon.com/ec2/instance-types/ ')\n",
    "    \n",
    "    return processes_per_host\n",
    "\n",
    "instance_type = 'ml.g5.12xlarge'\n",
    "\n",
    "processes_per_host = get_processes_per_host(instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d891ac0-9405-4216-a44b-e82844f12994",
   "metadata": {},
   "source": [
    "#### Point to an AWS-managed Deep Learning Container\n",
    "At AWS we provide 70+ prebuilt containers that are battle-tested, and known to run efficiently across SageMaker instances and features.\n",
    "\n",
    "Available images are listed here: https://github.com/aws/deep-learning-containers/blob/master/available_images.md \n",
    "\n",
    "You're welcome to bring your own Dockerfile, and either extend an AWS Deep Learning Container, or simply add the [sagemaker-training toolkit](https://github.com/aws/sagemaker-training-toolkit) to enable remote training job features like script-mode, local mode, distributed training, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad137b-85f9-4d27-a099-73b1937bcb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.12.1-gpu-py38-cu113-ubuntu20.04-sagemaker'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f57b0-3355-4098-8d81-db9fcabba232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "def get_estimator(instance_type, processes_per_host):\n",
    "    \n",
    "    sess = sagemaker.Session()\n",
    "\n",
    "    role = sagemaker.get_execution_role()\n",
    "\n",
    "    est = HuggingFace(entry_point='finetune.py',\n",
    "                      source_dir='src',\n",
    "                      image_uri=image_uri,\n",
    "                      sagemaker_session=sess,\n",
    "                      role=role,\n",
    "                      instance_type=instance_type,\n",
    "                      keep_alive_time_in_seconds = 3600,\n",
    "                      # output_path = can define s3 output here,\n",
    "                      py_version='py38',\n",
    "                      base_job_name='stable-diffusion', \n",
    "                      instance_count=1,\n",
    "                      # all opt/ml paths point to SageMaker training \n",
    "                      hyperparameters={\n",
    "                        'pretrained_model_name_or_path':'/opt/ml/input/data/training/sd-base-model',\n",
    "                        'dataset_name':'/opt/ml/input/data/training/dataset.parquet',\n",
    "                        'caption_column':'caption',\n",
    "                        'image_column':'sm_key',\n",
    "                        'resolution':256,\n",
    "                        'mixed_precision':'fp16',\n",
    "                        'train_batch_size':2,\n",
    "                        'learning_rate': '1e-10',\n",
    "                        'max_train_steps':100,\n",
    "                        'num_train_epochs':1,\n",
    "                        'output_dir':'/opt/ml/model/sd-output-final',   \n",
    "                      },    \n",
    "                      distribution={\"mpi\":{\"enabled\":True,\"processes_per_host\":processes_per_host}})\n",
    "    return est\n",
    "\n",
    "est = get_estimator(instance_type, processes_per_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d860818-aaf2-48a3-b8c6-c29596631c33",
   "metadata": {},
   "source": [
    "#### Enable FastFile Mode to speed up your training job\n",
    "[`FastFile`](https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-sagemaker-fast-file-mode/) is an option to stream data to your training job, rather than copying from S3. It's great when you don't have more than one million files, but see a bottleneck in the start of your training job. The alternatives are copying from S3, which is slow, or [FSx for Lustre](https://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/), which is the recommend path for very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95720591-f022-4887-a00a-ec04c0eeccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker.inputs.TrainingInput(s3_train_channel, s3_data_type='S3Prefix', input_mode='FastFile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1bc59-94c2-4da5-bf86-2db36acd1985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please note training can take upwards of 25 minutes (13 minutes for saving the model). \n",
    "# only run this cell ONCE!\n",
    "est.fit(inputs=inputs, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e099d-c6e0-463c-8310-e70576af5651",
   "metadata": {},
   "source": [
    "### Step 3. Distributed Inference\n",
    "Next, we'll point to the model we just trained in the previous step and use it to spin up a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff47324-ae2c-4b72-a022-7afdddd8d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define from the S3 path if you need to manually point to your model artifact\n",
    "# SageMaker hosting will want to see the model artifact be wrapped in tar.gz format\n",
    "#model_data = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c027e-a551-4a63-b9d6-6e6aecdac60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# hard code point to an image we're hosting for this workshop\n",
    "image_uri = '911195073761.dkr.ecr.us-east-1.amazonaws.com/sd-inference-gpu:latest'\n",
    "\n",
    "est=HuggingFaceModel(role=role,\n",
    "                     py_version='py38',\n",
    "                     model_data=est.model_data,\n",
    "                     image_uri=image_uri,\n",
    "                     sagemaker_session= sess,\n",
    "                     # set this to the number of GPUs in the intance type you'd like to use\n",
    "                     model_server_workers= 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc9aad-3cac-450b-b3f1-f349425b5e2e",
   "metadata": {},
   "source": [
    "Deploy your model for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc523a5c-a6ea-4471-aac5-b46358781ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = est.deploy(instance_type='ml.g5.12xlarge',\n",
    "                  initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c6e1e-d072-441f-8643-fda77ee09ee2",
   "metadata": {},
   "source": [
    "Provide prompts for training. The first text argument is based on this current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df953476-7ea4-49a7-9d2b-056125cbb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [caption,'A photo of an astronaut riding a horse on mars', \n",
    "           'A dragonfruit wearing karate belt in the snow.', \n",
    "           'Teddy bear swimming at the Olympics 400m Butter-fly event.',\n",
    "           'A cute sloth holding a small glowing treasure chest.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef67e8-6054-474d-97ab-fb280ff3874c",
   "metadata": {},
   "source": [
    "For more parameters feel free to explore [here](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion), just add 'parameters':{'key':'value'} to the input dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e555b6-abae-4938-bef8-4cef75b61bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = [pred.predict({'inputs':prompt}) \\\n",
    "           for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1d934-5d93-4527-be55-e69a0eebe62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [output['images'][0] for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b33055-9f20-461d-ac6f-cb8d5db72f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "def process_result(out):\n",
    "    return Image.open(BytesIO(base64.b64decode(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5fbaf8-033f-4ac2-82d5-69f672e182f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [[process_result(output),prompt] for output,prompt in zip(outputs,prompts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d29c30-3c3b-42fe-b0e4-a82e25d75612",
   "metadata": {},
   "source": [
    "#### Visualize the results from the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20428b6-19dc-4aca-931d-a4f7d0460722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(len(images)):\n",
    "    plt.figure()\n",
    "    plt.title(images[i][1])\n",
    "    plt.imshow(images[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630ce29-cf41-41fe-928c-040124508064",
   "metadata": {},
   "source": [
    "#### Generate images from text\n",
    "Now let's test the results line by line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccc748-ad93-4490-a934-e8659952d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a beautiful hot arabian desert\"\n",
    "\n",
    "output = pred.predict({'inputs':prompt})\n",
    "process_result(output['images'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2e280-3b01-48b9-8b15-7202b9366eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a delicious arabian dessert\"\n",
    "\n",
    "output = pred.predict({'inputs':prompt})\n",
    "process_result(output['images'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29c9c1-137a-4246-88bc-0d2e63ede648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up your endpoint\n",
    "# pred.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bd07f-8fbd-4a11-9ce8-810ac1a3d906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
